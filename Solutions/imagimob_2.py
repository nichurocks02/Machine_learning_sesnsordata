# -*- coding: utf-8 -*-
"""imagimob_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16h6dXGJ--AZABSfKmGpjv8l9kH7yknF7
"""

#importing library files 
import sys
import io
import pandas as pd 
import numpy as np
import time
import matplotlib.pyplot as plt
from matplotlib.pyplot import figure
import seaborn as sns
from sklearn import preprocessing
from sklearn.metrics import classification_report,confusion_matrix
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans

#reading the data from the file and performing few separation operations  
df = pd.read_csv('file_name.csv') #please enter the full extension of the file to be uploaded
time = df['Time(Seconds)'].values

#performing preprocessing by normalizing the values of sensor data and leaving apart the time value
x = df[['Time(Seconds)','gyroX[mdps]',	'gyroY[mdps]',	'gyroZ[mdps]',	'accX[mg]',	'accY[mg]',	'accZ[mg]']].values.astype(float)

# Create a minimum and maximum processor object
min_max_scaler = preprocessing.MinMaxScaler()

# Create an object to transform the required data to fit minmax processor for selected features
x_scaled = min_max_scaler.fit_transform(x[:,1:])

# Run the normalizer on the dataframe 
df_normalized = pd.DataFrame(x_scaled)
#renaming the columns of the data frame
df_normalized.columns = ['gyroX[mdps]',	'gyroY[mdps]',	'gyroZ[mdps]',	'accX[mg]',	'accY[mg]',	'accZ[mg]']
print('this data frame represents the normalized data values of sensor value')
print(df_normalized)

#individually plotting the features i.e. the sensor data with respect to time 
df_normalized1= df_normalized
df_normalized1['Time(Seconds)'] = time
df_normalized1 = df_normalized1[['Time(Seconds)','gyroX[mdps]',	'gyroY[mdps]',	'gyroZ[mdps]',	'accX[mg]',	'accY[mg]',	'accZ[mg]']]
print('Below is the time series graph of individual feature')
print(df_normalized1.plot(x='Time(Seconds)',subplots = True,style = 'k.'))
print(plt.show())

#as from the above plots we can see there are outliers, below i try to trim them in order to get analysis or extract features of important data
# We use Z analysis method from statistics to identify the outliers and winsorating them with the nearest value in our case the mean value of the feature
outlier = {} # dictionary to store the outliers of individual features
columns = ['gyroX[mdps]',	'gyroY[mdps]',	'gyroZ[mdps]',	'accX[mg]',	'accY[mg]',	'accZ[mg]']
main_list=[]
for j in columns:
  list1 =[]
  mean = np.mean(df_normalized[j])
  std = np.std(df_normalized[j])
  for i in range(len((df_normalized[j]))):
    z = (df_normalized[j][i]-mean)/std 
    if z > 3: # check for the condition if the z value i.e original value - mean /standard deviation 
      list1.append(z)
      df_normalized[j][i] = mean 
  main_list.append(list1)        
print(main_list)
for j in range(len(columns)):
  outlier[j]=main_list[j]

print(outlier)

# This section of code prints the outliers present in the individual features
final_outlier = dict(zip(columns, list(outlier.values()))) 
print("Below is the dictionary of the outlier values of different features ")
print(str(final_outlier))
print(str(final_outlier['gyroX[mdps]']))
print(str(final_outlier['gyroY[mdps]']))
print(str(final_outlier['gyroZ[mdps]']))
print(str(final_outlier['accX[mg]']))
print(str(final_outlier['accY[mg]']))
print(str(final_outlier['accZ[mg]']))

#Performing dimensionality reduction for ease of analysis
#PCA is very sensitive to outliers,hence it as outliers were truncated in the previous steps.
#PCA helps maximize the variances by minimizing the reconstruction error.
pca = PCA(n_components=1) # n no of components is selected as 1 with reference to gryoscope and accelometer sensor data
X_pca = pca.fit_transform(df_normalized)
PCA_df = pd.DataFrame(data = X_pca, columns = ['PC1'])
PCA_df['Time(Seconds)'] = time
PCA_df = PCA_df[['Time(Seconds)','PC1']]
print('Below is the dataset after dimensionality reduction')
print('****************************************************')
print(PCA_df)

# here we perform KMeans clustering - i.e. unsupervised clustering algorithm after performing PCA in order to extract important features into groups so that it is easy for further analysis
# as kmeans is sensitive to intialization we use kmeans++ for that purpose
#inorder to find the optimal number of clusters required we consider wcss 
#Within-Cluster-Sum-of-Squares helps find the distance of each data point of cluster w.r.t it's centroid
data = PCA_df.iloc[:,0:2] # differentianting the sensor data after performing dimensionality reduction
wcss = [] #empty list for calculating wcss after each iter
for i in range(1,100):
  kmeans =KMeans(i) 
  kmeans.fit(data) # intially time is also being considered for segregating into groups 
  wcss_iter = kmeans.inertia_
  wcss.append(wcss_iter)

print(wcss)

# finding out the optimal number of clusters required using elbow method
number_of_clusters = range(1,100)
plt.plot(number_of_clusters,wcss)
plt.title('The Elbow method')
plt.xlabel('Number of clusters')
plt.ylabel('Within-cluster-sum-of-squares')

#results from clustering 
kmeans_new = KMeans(20)
identified_clusters = kmeans_new.fit_predict(data) #predicting dependent of time
cluster_data = PCA_df.copy()
cluster_data['Cluster'] = identified_clusters
print('identified cluster groups labels')
print('************************************')
print(cluster_data)
print('************************************')

# Now we try plotting the scatter plot of the clustered data with respect to time
cluster_data['Time(Seconds)']=cluster_data['Time(Seconds)']/3600
plt.scatter(cluster_data['Time(Seconds)'],cluster_data['PC1'],c=cluster_data['Cluster'],cmap='rainbow')
plt.xlabel('time in hours')
plt.ylabel('sensor values after PCA')
plt.show()

#above you can the features categorized based on time and dimenionally reduced data of sensors